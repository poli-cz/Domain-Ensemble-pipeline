{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da08d06-be0d-468a-bd17-d7bcba74880a",
   "metadata": {},
   "source": [
    "## Columns to be removed from training/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715b0f7",
   "metadata": {},
   "source": [
    "# Load Tensorflow and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a3b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 16:00:06.197563: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-24 16:00:06.197594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-24 16:00:06.216772: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-24 16:00:06.271533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 16:00:07.340989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 16:00:09.114893: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:09.248939: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:09.251754: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:10.300711: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:10.302131: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:10.303340: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:10.304706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 2031 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from core.loader import Loader\n",
    "from models.model_wrapper import ModelWrapper\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(device.physical_device_desc)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1258a0e",
   "metadata": {},
   "source": [
    "# Load input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069058d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_dataset_filenames = [\n",
    "    'parkets/benign/benign_2312_anonymized_HTML.parquet', \n",
    "    'parkets/benign/umbrella_benign_FINISHED_HTML.parquet'\n",
    "        \n",
    "]\n",
    "malicious_dataset_filenames = [\n",
    "    'parkets/malware_2406_strict_HTML.parquet'\n",
    "]\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "benign_label = \"benign\"\n",
    "malicious_label = \"malware\"\n",
    "\n",
    "class_map = {benign_label: 0, malicious_label: 1}\n",
    "# print labels from malicious datasets\n",
    "\n",
    "loader = Loader(benign_dataset_filenames, malicious_dataset_filenames, benign_label=benign_label, malicious_label=malicious_label, subsample=1.0)\n",
    "df = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca51639",
   "metadata": {},
   "source": [
    "# split into 3 stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b37550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poli/Desktop/git/deep_domain_detection/src/core/segmenter.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/segmenter.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/segmenter.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/segmenter.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/segmenter.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/segmenter.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/segmenter.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n"
     ]
    }
   ],
   "source": [
    "from core.segmenter import Segmenter\n",
    "\n",
    "# Define the aggregates that needs to be created\n",
    "\n",
    "aggregates = [\n",
    "    [\"lex_\"],\n",
    "    [\"lex_\", \"dns_\", \"ip_\", \"geo_\"],\n",
    "    [\"lex_\", \"dns_\", \"ip_\", \"tls_\", \"geo_\", \"rdap_\"],\n",
    "]\n",
    "\n",
    "segmenter = Segmenter(df)\n",
    "segmenter.create_base_subsets() # create base subsets\n",
    "segmenter.create_aggregated_subsets(aggregates)\n",
    "subset_dfs = segmenter.get_aggregated_subsets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e103a79",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87705691",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['label'].apply(lambda x: class_map[x]) # y vector\n",
    "features = df.drop('label', axis=1).copy() # X matrix\n",
    "\n",
    "features = loader.scale(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fd630",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "  features,\n",
    "  labels,\n",
    "  test_size=0.2,\n",
    "  random_state=42,\n",
    "  shuffle=True, \n",
    "  stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecd4d4",
   "metadata": {},
   "source": [
    "# Define the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ecaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# TODO: Testovat NN na prvni 3 stupne. \n",
    "# TODO: Pro kaÅ¾dÃ½ stupeÅˆ vybrat nejlepÅ¡Ã­ model\n",
    "\n",
    "# TODO: MÄ›Å™it na celÃ©m datasetu + potom udÄ›lat validaci na CLF testu\n",
    "\n",
    "\n",
    "ARCH_NAME = \"feedforward\"\n",
    "VERSION = \"v1.0\"\n",
    "LR = 0.0023\n",
    "\n",
    "def build_feedforward_net(feature_size):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(feature_size,))\n",
    "    \n",
    "    # First hidden layer\n",
    "    \n",
    "    x = Dense(1024, activation=None)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    x = Dense(512, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Third hidden layer\n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Fourth hidden layer\n",
    "    x = Dense(128, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build and return the model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"ARCH_NAME\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a66df97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 16:00:32.622251: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:32.626292: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:32.629065: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:32.631912: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:32.634660: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:32.637306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 2031 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# save subset_dfs to pkl file to tmp folder\n",
    "import os\n",
    "import tempfile\n",
    "import pickle\n",
    "import torch\n",
    "from core.loader import Loader\n",
    "from models.model_wrapper import ModelWrapper\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(device.physical_device_desc)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_file = \"subset_dfs_malware.pkl\"\n",
    "tmp_dir_name = \"tmp\"\n",
    "\n",
    "\n",
    "tmp_dir = os.path.join(os.getcwd(), tmp_dir_name)\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)\n",
    "tmp_file = os.path.join(tmp_dir, save_file)\n",
    "\n",
    "def save_subset_dfsp(subset_dfs):\n",
    "    with open(tmp_file, 'wb') as f:\n",
    "        pickle.dump(subset_dfs, f)\n",
    "    return tmp_file\n",
    "\n",
    "def load_subset_dfs_from_tmp(tmp_file):\n",
    "    with open(tmp_file, 'rb') as f:\n",
    "        subset_dfs = pickle.load(f)\n",
    "    return subset_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed10662",
   "metadata": {},
   "source": [
    "### Save subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dcd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save subsets\n",
    "save_subset_dfsp(subset_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06670977",
   "metadata": {},
   "source": [
    "### Load subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e65512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subsets \n",
    "subset_dfs = load_subset_dfs_from_tmp(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a587cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training Feedforward NN on 'lex_agg' featuresâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 16:00:44.829796: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:44.832917: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 16:00:44.835682: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix, subset_df \u001b[38;5;129;01min\u001b[39;00m subset_dfs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ Training Feedforward NN on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m featuresâ€¦\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     labels   \u001b[38;5;241m=\u001b[39m subset_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[43mclass_map\u001b[49m)\n\u001b[1;32m     21\u001b[0m     features \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mscale(subset_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     23\u001b[0m     X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     24\u001b[0m         features, labels,\n\u001b[1;32m     25\u001b[0m         test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     26\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, stratify\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     27\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_map' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.model_wrapper import ModelWrapper\n",
    "\n",
    "\n",
    "model_histories = []\n",
    "\n",
    "# make sure TF only allocates as much GPU memory as it needs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "wrapper = ModelWrapper()\n",
    "\n",
    "for prefix, subset_df in subset_dfs.items():\n",
    "\n",
    "    print(f\"\\nðŸš€ Training Feedforward NN on '{prefix}' featuresâ€¦\")\n",
    "    labels   = subset_df['label'].map(class_map)\n",
    "    features = loader.scale(subset_df.drop('label', axis=1))\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        features, labels,\n",
    "        test_size=0.2, random_state=42,\n",
    "        shuffle=True, stratify=labels\n",
    "    )\n",
    "\n",
    "    model = build_feedforward_net(X_train.shape[1])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LR),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['Precision', 'Recall', 'AUC']\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        batch_size=512,\n",
    "        epochs=25,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        class_weight={0: 1.0, 1: 1.0},\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "\n",
    "    model_histories.append({\"model_name\": prefix,\n",
    "                      \"history\": history})\n",
    "    \n",
    "\n",
    "    wrapper.save(model,\n",
    "                 arch_name=ARCH_NAME,\n",
    "                 label=malicious_label,\n",
    "                 prefix=prefix,\n",
    "                 version=VERSION)\n",
    "\n",
    "    # ---- hereâ€™s the magic ----\n",
    "    K.clear_session()    # drops the entire TF graph + variables\n",
    "    del model           # remove the Python reference\n",
    "    del history         # free training history\n",
    "    del X_train, X_test, Y_train, Y_test, features, labels\n",
    "    gc.collect()        # ask Python to free unreferenced memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6111038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(history, metric_name):\n",
    "    if metric_name in history.history:\n",
    "        return history.history[metric_name]\n",
    "    elif f\"{metric_name}_12\" in history.history:\n",
    "        return history.history[f\"{metric_name}_12\"]\n",
    "    elif f\"{metric_name}_2\" in history.history:\n",
    "        return history.history[f\"{metric_name}_2\"]\n",
    "    else:\n",
    "        raise KeyError(f\"Metric {metric_name} not found in history.\")\n",
    "\n",
    "epoch_losses = get_metric(history, 'loss')\n",
    "epoch_val_losses = get_metric(history, 'val_loss')\n",
    "epoch_accuracies = get_metric(history, 'auc')\n",
    "epoch_val_accuracies = get_metric(history, 'val_auc')\n",
    "epoch_precisions = get_metric(history, 'precision')\n",
    "epoch_val_precisions = get_metric(history, 'val_precision')\n",
    "epoch_recalls = get_metric(history, 'recall')\n",
    "epoch_val_recalls = get_metric(history, 'val_recall')\n",
    "\n",
    "# Calculate F1 score\n",
    "epoch_f1s = [2 * (p * r) / (p + r) for p, r in zip(epoch_precisions, epoch_recalls)]\n",
    "epoch_val_f1s = [2 * (p * r) / (p + r) for p, r in zip(epoch_val_precisions, epoch_val_recalls)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "# Plot for Loss\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(epoch_losses, linestyle='--', marker='o', color='b', label='Training Loss')\n",
    "plt.plot(epoch_val_losses, linestyle='--', marker='o', color='r', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for AUC (as a proxy for Accuracy)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(epoch_accuracies, linestyle='--', marker='o', color='#ff7f0e', label='Training AUC')\n",
    "plt.plot(epoch_val_accuracies, linestyle='--', marker='o', color='r', label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC (Accuracy)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for Precision\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(epoch_precisions, linestyle='--', marker='o', color='g', label='Training Precision')\n",
    "plt.plot(epoch_val_precisions, linestyle='--', marker='o', color='r', label='Validation Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for Recall\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(epoch_recalls, linestyle='--', marker='o', color='c', label='Training Recall')\n",
    "plt.plot(epoch_val_recalls, linestyle='--', marker='o', color='r', label='Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for F1 Score\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(epoch_f1s, linestyle='--', marker='o', color='m', label='Training F1 Score')\n",
    "plt.plot(epoch_val_f1s, linestyle='--', marker='o', color='r', label='Validation F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.suptitle('NN Training Progress', fontsize=16, y=1.02)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f'./figures/training_{ARCH_NAME}_{malicious_label}_{VERSION}.png', dpi=500, bbox_inches='tight', pad_inches=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb708ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the model\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ', loss_and_metrics[0])\n",
    "print('Accuracy = ', loss_and_metrics[1])\n",
    "\n",
    "# Generate predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = np.round(Y_pred).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# False Positive Rate\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "# Display the metrics\n",
    "print('\\n=== RESULTS ===')\n",
    "print(classification_report(Y_test, Y_pred, target_names=['Benign', 'Malicious'], digits=4))\n",
    "print('False Positive Rate =', fpr)\n",
    "\n",
    "\n",
    "# Display the confusion matrix\n",
    "print('\\nConfusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Optionally, plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))  # Increase figure size for better readability\n",
    "disp.plot(ax=ax, values_format='d')\n",
    "for labels in disp.text_:\n",
    "    for label in labels:\n",
    "        label.set_fontsize(18) \n",
    "plt.show()\n",
    "\n",
    "# TODO: ADD clf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3d6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze_feature_importance = True\n",
    "\n",
    "if analyze_feature_importance:\n",
    "    \n",
    "    import shap\n",
    "    \n",
    "    # Ensure that X_train and X_test are DataFrames with the correct column names\n",
    "    # You can set the column names from the 'features' DataFrame like this:\n",
    "    X_train.columns = features.columns\n",
    "    X_test.columns = features.columns\n",
    "    \n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Convert your training set to a NumPy format if it's not already\n",
    "    background = X_train[:n_samples].to_numpy()\n",
    "    \n",
    "    # Use the generic SHAP Explainer interface\n",
    "    explainer = shap.Explainer(model, background)\n",
    "    \n",
    "    # Generate SHAP values for the test set\n",
    "    shap_values = explainer(X_test[:n_samples].to_numpy(), max_evals=1000)\n",
    "    \n",
    "    # Plotting the summary plot for feature importance\n",
    "    # Use the column names from the 'features' DataFrame as the feature names\n",
    "    shap.summary_plot(shap_values.values, X_test[:n_samples], feature_names=features.columns, max_display=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
