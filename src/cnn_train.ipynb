{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da08d06-be0d-468a-bd17-d7bcba74880a",
   "metadata": {},
   "source": [
    "## Columns to be removed from training/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715b0f7",
   "metadata": {},
   "source": [
    "# Load Tensorflow and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a3b996",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Loader\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelWrapper\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_lib\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from ..core.loader import Loader\n",
    "from ..models.model_wrapper import ModelWrapper\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(device.physical_device_desc)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1258a0e",
   "metadata": {},
   "source": [
    "# Load input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069058d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_dataset_filenames = [\n",
    "    'parkets/benign/benign_2312_anonymized_HTML.parquet', \n",
    "    'parkets/benign/umbrella_benign_FINISHED_HTML.parquet'\n",
    "        \n",
    "]\n",
    "malicious_dataset_filenames = [\n",
    "    'parkets/phishing_2406_strict_HTML.parquet'\n",
    "]\n",
    "\n",
    "\n",
    "# print me number of domains from each separate dataset\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "benign_label = \"benign\"\n",
    "malicious_label = \"malware\"\n",
    "\n",
    "class_map = {benign_label: 0, malicious_label: 1}\n",
    "# print labels from malicious datasets\n",
    "\n",
    "loader = Loader(benign_dataset_filenames, malicious_dataset_filenames, benign_label=benign_label, malicious_label=malicious_label, subsample=0.6)\n",
    "df = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca51639",
   "metadata": {},
   "source": [
    "# split into 3 stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b37550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.loader import Segmenter\n",
    "\n",
    "# Define the aggregates that needs to be created\n",
    "\n",
    "aggregates = [\n",
    "    [\"lex_\"],\n",
    "    [\"lex_\", \"dns_\", \"ip_\", \"geo_\"],\n",
    "    [\"lex_\", \"dns_\", \"ip_\", \"tls_\", \"geo_\", \"rdap_\"],\n",
    "]\n",
    "\n",
    "segmenter = Segmenter(df)\n",
    "segmenter.create_base_subsets() # create base subsets\n",
    "segmenter.create_aggregated_subsets(aggregates)\n",
    "subset_dfs = segmenter.get_aggregated_subsets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecd4d4",
   "metadata": {},
   "source": [
    "# Define the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ecaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, BatchNormalization, Activation, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "ARCH_NAME = \"cnn\"\n",
    "VERSION = \"v1.1\"\n",
    "LR = 0.0023\n",
    "\n",
    "def build_cnn_net(input_shape=(28, 28, 1), dropout_conv=0.25, dropout_dense=0.5):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(32, kernel_size=(3, 3), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(64, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Dense block 1\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_dense)(x)\n",
    "\n",
    "    # Dense block 2\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_dense)(x)\n",
    "\n",
    "    # Output\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=ARCH_NAME)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed10662",
   "metadata": {},
   "source": [
    "### Save subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a587cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.model_wrapper import ModelWrapper\n",
    "\n",
    "def next_perfect_square(n):\n",
    "    return int(np.ceil(np.sqrt(n)) ** 2)\n",
    "\n",
    "model_histories = []\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "wrapper = ModelWrapper()\n",
    "i = 1\n",
    "\n",
    "for prefix, subset_df in subset_dfs.items():\n",
    "    if i < 3:\n",
    "        print(\"We need only the last subset \")\n",
    "        i+=1\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nðŸš€ Training CNN on '{prefix}' featuresâ€¦\")\n",
    "    labels = subset_df['label'].map(class_map)\n",
    "    features_df = loader.scale(subset_df.drop('label', axis=1), stage=i, model=ARCH_NAME)\n",
    "\n",
    "    features = features_df.values\n",
    "    original_feature_size = features.shape[1]\n",
    "    padded_size = next_perfect_square(original_feature_size)\n",
    "    side_size = int(np.sqrt(padded_size))\n",
    "    padding = padded_size - original_feature_size\n",
    "\n",
    "    if padding > 0:\n",
    "        features = np.pad(features, ((0, 0), (0, padding)), mode='constant', constant_values=0)\n",
    "\n",
    "    X = features.reshape(-1, side_size, side_size, 1)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, labels,\n",
    "        test_size=0.2, random_state=42,\n",
    "        shuffle=True, stratify=labels\n",
    "    )\n",
    "\n",
    "    model = build_cnn_net(input_shape=(side_size, side_size, 1))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LR),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['Precision', 'Recall', 'AUC']\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        batch_size=512,\n",
    "        epochs=30,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        class_weight={0: 1.0, 1: 0.8},\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    model_histories.append({\"model_name\": prefix, \"history\": history})\n",
    "\n",
    "    wrapper.save(model,\n",
    "                 arch_name=ARCH_NAME,\n",
    "                 label=malicious_label,\n",
    "                 prefix=prefix,\n",
    "                 version=VERSION)\n",
    "\n",
    "    K.clear_session()\n",
    "    del model, history, X_train, X_test, Y_train, Y_test, X, features, labels\n",
    "    gc.collect()\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6111038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume model_histories is a list of dicts, each with keys \"model_name\" and \"history\"\n",
    "def get_metric(history, metric_name):\n",
    "    if metric_name in history:\n",
    "        return history[metric_name]\n",
    "    for suffix in [\"_12\", \"_2\"]:\n",
    "        if f\"{metric_name}{suffix}\" in history:\n",
    "            return history[f\"{metric_name}{suffix}\"]\n",
    "    raise KeyError(f\"Metric {metric_name} not found in history.\")\n",
    "\n",
    "for model_entry in model_histories:\n",
    "    name = model_entry[\"model_name\"]\n",
    "    history = model_entry[\"history\"].history  # Keras history object\n",
    "\n",
    "    epoch_losses = get_metric(history, 'loss')\n",
    "    epoch_val_losses = get_metric(history, 'val_loss')\n",
    "    epoch_accuracies = get_metric(history, 'auc')\n",
    "    epoch_val_accuracies = get_metric(history, 'val_auc')\n",
    "    epoch_precisions = get_metric(history, 'precision')\n",
    "    epoch_val_precisions = get_metric(history, 'val_precision')\n",
    "    epoch_recalls = get_metric(history, 'recall')\n",
    "    epoch_val_recalls = get_metric(history, 'val_recall')\n",
    "\n",
    "    # Calculate F1 scores\n",
    "    def safe_f1(p, r):\n",
    "        return 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "    epoch_f1s = [safe_f1(p, r) for p, r in zip(epoch_precisions, epoch_recalls)]\n",
    "    epoch_val_f1s = [safe_f1(p, r) for p, r in zip(epoch_val_precisions, epoch_val_recalls)]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epoch_losses, 'b--o', label='Training Loss')\n",
    "    plt.plot(epoch_val_losses, 'r--o', label='Validation Loss')\n",
    "    plt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epoch_accuracies, '--o', label='Training AUC', color='#ff7f0e')\n",
    "    plt.plot(epoch_val_accuracies, 'r--o', label='Validation AUC')\n",
    "    plt.title('AUC'); plt.xlabel('Epoch'); plt.ylabel('AUC'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epoch_precisions, 'g--o', label='Training Precision')\n",
    "    plt.plot(epoch_val_precisions, 'r--o', label='Validation Precision')\n",
    "    plt.title('Precision'); plt.xlabel('Epoch'); plt.ylabel('Precision'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epoch_recalls, 'c--o', label='Training Recall')\n",
    "    plt.plot(epoch_val_recalls, 'r--o', label='Validation Recall')\n",
    "    plt.title('Recall'); plt.xlabel('Epoch'); plt.ylabel('Recall'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(epoch_f1s, 'm--o', label='Training F1')\n",
    "    plt.plot(epoch_val_f1s, 'r--o', label='Validation F1')\n",
    "    plt.title('F1 Score'); plt.xlabel('Epoch'); plt.ylabel('F1'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.suptitle(f\"Training Progress - {name}\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(f'./figures/training_{ARCH_NAME}_{name}_{VERSION}.png', dpi=500, bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
