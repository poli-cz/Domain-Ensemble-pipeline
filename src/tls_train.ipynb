{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da08d06-be0d-468a-bd17-d7bcba74880a",
   "metadata": {},
   "source": [
    "## Columns to be removed from training/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715b0f7",
   "metadata": {},
   "source": [
    "# Load Tensorflow and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a3b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 12:17:47.976463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-13 12:17:47.976495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-13 12:17:47.977510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-13 12:17:47.983175: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 12:17:48.787941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 12:17:49.607330: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "/home/poli/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2025-05-13 12:17:49.607353: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: poli\n",
      "2025-05-13 12:17:49.607358: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: poli\n",
      "2025-05-13 12:17:49.607392: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 570.133.7\n",
      "2025-05-13 12:17:49.607411: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: NOT_FOUND: could not find kernel module information in driver version file contents: \"NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  570.86.10  Release Build  (dvs-builder@U16-I3-B16-4-3)  Thu Jan 16 03:40:42 UTC 2025\n",
      "GCC version:  gcc version 12.3.0 (Ubuntu 12.3.0-1ubuntu1~22.04) \n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from core.loader import Loader\n",
    "from models.model_wrapper import ModelWrapper\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(device.physical_device_desc)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from core.loader import Segmenter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1258a0e",
   "metadata": {},
   "source": [
    "# Load input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069058d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_dataset_filenames = [\n",
    "    'parkets/benign/benign_2312_anonymized_HTML.parquet', \n",
    "    'parkets/benign/umbrella_benign_FINISHED_HTML.parquet'\n",
    "        \n",
    "]\n",
    "malicious_dataset_filenames = [\n",
    "    'parkets/malware_2406_strict_HTML.parquet'\n",
    "]\n",
    "\n",
    "\n",
    "# print me number of domains from each separate dataset\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "benign_label = \"benign\"\n",
    "malicious_label = \"malware\"\n",
    "\n",
    "class_map = {benign_label: 0, malicious_label: 1}\n",
    "# print labels from malicious datasets\n",
    "\n",
    "loader = Loader(benign_dataset_filenames, malicious_dataset_filenames, benign_label=benign_label, malicious_label=malicious_label, subsample=1.0)\n",
    "df = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca51639",
   "metadata": {},
   "source": [
    "# split into 3 stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b37550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n"
     ]
    }
   ],
   "source": [
    "from core.loader import Segmenter\n",
    "\n",
    "# Define the aggregates that needs to be created\n",
    "\n",
    "aggregates = [\n",
    "    [\"lex_\", \"dns_\", \"ip_\", \"tls_\", \"geo_\", \"rdap_\"],\n",
    "]\n",
    "\n",
    "segmenter = Segmenter(df)\n",
    "segmenter.create_base_subsets() # create base subsets\n",
    "segmenter.create_aggregated_subsets(aggregates)\n",
    "subset_dfs = segmenter.get_aggregated_subsets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecd4d4",
   "metadata": {},
   "source": [
    "# Define the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ecaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, BatchNormalization, ReLU, Dropout,\n",
    "    Multiply, Add\n",
    ")\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "\n",
    "ARCH_NAME = \"attention_tls\"\n",
    "LR = 0.0023  # Slightly lower for better convergence\n",
    "VERSION = \"v1.1\"\n",
    "\n",
    "def build_tls_classifier(input_dim):\n",
    "    inputs = Input(shape=(input_dim,), name=\"input\")\n",
    "\n",
    "    # Attention mechanism\n",
    "    attention_weights = Dense(input_dim, activation=\"sigmoid\")(inputs)\n",
    "    attention_applied = Multiply(name=\"attention_applied\")([inputs, attention_weights])\n",
    "\n",
    "    # Input projection\n",
    "    x = Dense(512, name=\"dense_512\")(attention_applied)\n",
    "    x = BatchNormalization(name=\"bn_512\")(x)\n",
    "    x = ReLU(name=\"relu_512\")(x)\n",
    "\n",
    "    # First hidden layer\n",
    "    x = Dense(256, name=\"dense_256\")(x)\n",
    "    x = BatchNormalization(name=\"bn_256\")(x)\n",
    "    x = ReLU(name=\"relu_256\")(x)\n",
    "    x = Dropout(0.3, name=\"dropout_256\")(x)\n",
    "\n",
    "    # Second hidden layer\n",
    "    x = Dense(128, name=\"dense_128\")(x)\n",
    "    x = BatchNormalization(name=\"bn_128\")(x)\n",
    "    x = ReLU(name=\"relu_128\")(x)\n",
    "    x = Dropout(0.3, name=\"dropout_128\")(x)\n",
    "\n",
    "    # Skip connection from input\n",
    "    skip = Dense(128, activation=\"relu\", name=\"skip_connection\")(inputs)\n",
    "\n",
    "    # Add skip connection\n",
    "    x = Add(name=\"add_skip\")([x, skip])\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"tls_classifier\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed10662",
   "metadata": {},
   "source": [
    "### Save subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06670977",
   "metadata": {},
   "source": [
    "### Load subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a587cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training Feedforward NN on 'lex_+dns_+ip_+tls_+geo_+rdap_agg' featuresâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 13:17:52.629969: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1104715392 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1533/1533 [==============================] - 12s 6ms/step - loss: 0.0516 - precision: 0.9597 - recall: 0.9018 - auc: 0.9930 - val_loss: 0.0444 - val_precision: 0.9743 - val_recall: 0.9338 - val_auc: 0.9960\n",
      "Epoch 2/15\n",
      "1533/1533 [==============================] - 8s 5ms/step - loss: 0.0331 - precision: 0.9762 - recall: 0.9391 - auc: 0.9962 - val_loss: 0.0393 - val_precision: 0.9836 - val_recall: 0.9370 - val_auc: 0.9965\n",
      "Epoch 3/15\n",
      "1533/1533 [==============================] - 8s 5ms/step - loss: 0.0288 - precision: 0.9796 - recall: 0.9468 - auc: 0.9969 - val_loss: 0.0356 - val_precision: 0.9759 - val_recall: 0.9524 - val_auc: 0.9969\n",
      "Epoch 4/15\n",
      "1533/1533 [==============================] - 8s 5ms/step - loss: 0.0257 - precision: 0.9824 - recall: 0.9524 - auc: 0.9973 - val_loss: 0.0368 - val_precision: 0.9852 - val_recall: 0.9423 - val_auc: 0.9963\n",
      "Epoch 5/15\n",
      "1533/1533 [==============================] - 8s 6ms/step - loss: 0.0236 - precision: 0.9836 - recall: 0.9561 - auc: 0.9975 - val_loss: 0.0336 - val_precision: 0.9823 - val_recall: 0.9523 - val_auc: 0.9964\n",
      "Epoch 6/15\n",
      "1533/1533 [==============================] - 8s 5ms/step - loss: 0.0218 - precision: 0.9851 - recall: 0.9598 - auc: 0.9978 - val_loss: 0.0334 - val_precision: 0.9829 - val_recall: 0.9510 - val_auc: 0.9971\n",
      "Epoch 7/15\n",
      "1533/1533 [==============================] - 8s 5ms/step - loss: 0.0203 - precision: 0.9862 - recall: 0.9622 - auc: 0.9981 - val_loss: 0.0320 - val_precision: 0.9821 - val_recall: 0.9583 - val_auc: 0.9968\n",
      "Epoch 8/15\n",
      "1533/1533 [==============================] - 8s 6ms/step - loss: 0.0189 - precision: 0.9871 - recall: 0.9649 - auc: 0.9983 - val_loss: 0.0319 - val_precision: 0.9841 - val_recall: 0.9553 - val_auc: 0.9966\n",
      "Epoch 9/15\n",
      "1533/1533 [==============================] - 9s 6ms/step - loss: 0.0178 - precision: 0.9879 - recall: 0.9669 - auc: 0.9985 - val_loss: 0.0320 - val_precision: 0.9779 - val_recall: 0.9613 - val_auc: 0.9968\n",
      "Epoch 10/15\n",
      "1533/1533 [==============================] - 8s 6ms/step - loss: 0.0168 - precision: 0.9885 - recall: 0.9689 - auc: 0.9986 - val_loss: 0.0337 - val_precision: 0.9865 - val_recall: 0.9535 - val_auc: 0.9958\n",
      "Epoch 11/15\n",
      "1533/1533 [==============================] - 8s 5ms/step - loss: 0.0156 - precision: 0.9894 - recall: 0.9703 - auc: 0.9987 - val_loss: 0.0340 - val_precision: 0.9789 - val_recall: 0.9619 - val_auc: 0.9960\n",
      "Epoch 12/15\n",
      "1533/1533 [==============================] - 9s 6ms/step - loss: 0.0149 - precision: 0.9897 - recall: 0.9720 - auc: 0.9988 - val_loss: 0.0328 - val_precision: 0.9837 - val_recall: 0.9598 - val_auc: 0.9959\n",
      "Epoch 13/15\n",
      "1533/1533 [==============================] - 8s 5ms/step - loss: 0.0142 - precision: 0.9902 - recall: 0.9735 - auc: 0.9989 - val_loss: 0.0342 - val_precision: 0.9796 - val_recall: 0.9631 - val_auc: 0.9958\n",
      "Epoch 14/15\n",
      "1533/1533 [==============================] - 9s 6ms/step - loss: 0.0133 - precision: 0.9907 - recall: 0.9752 - auc: 0.9990 - val_loss: 0.0335 - val_precision: 0.9801 - val_recall: 0.9628 - val_auc: 0.9960\n",
      "Epoch 15/15\n",
      "1533/1533 [==============================] - 9s 6ms/step - loss: 0.0128 - precision: 0.9909 - recall: 0.9755 - auc: 0.9991 - val_loss: 0.0335 - val_precision: 0.9810 - val_recall: 0.9629 - val_auc: 0.9954\n",
      "Saving as stage: stage_3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.model_wrapper import ModelWrapper\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_histories = []\n",
    "\n",
    "# make sure TF only allocates as much GPU memory as it needs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "wrapper = ModelWrapper()\n",
    "\n",
    "\n",
    "for prefix, subset_df in subset_dfs.items():\n",
    "\n",
    "\n",
    "    print(f\"\\nðŸš€ Training Feedforward NN on '{prefix}' featuresâ€¦\")\n",
    "    labels   = subset_df['label'].map(class_map)\n",
    "    features = loader.scale(subset_df.drop('label', axis=1), stage=3, model=ARCH_NAME)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        features, labels,\n",
    "        test_size=0.2, random_state=42,\n",
    "        shuffle=True, stratify=labels\n",
    "    )\n",
    "\n",
    "    model = build_tls_classifier(X_train.shape[1])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['Precision', 'Recall', 'AUC']\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        batch_size=512,\n",
    "        epochs=15,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        class_weight={0: 1.0, 1: 0.7},\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "\n",
    "    model_histories.append({\"model_name\": prefix,\n",
    "                      \"history\": history})\n",
    "    \n",
    "\n",
    "    wrapper.save(model,\n",
    "                 arch_name=ARCH_NAME,\n",
    "                 label=malicious_label,\n",
    "                 prefix=prefix,\n",
    "                 version=VERSION)\n",
    "\n",
    "    # ---- hereâ€™s the magic ----\n",
    "    K.clear_session()    # drops the entire TF graph + variables\n",
    "    del model           # remove the Python reference\n",
    "    del history         # free training history\n",
    "    del X_train, X_test, Y_train, Y_test, features, labels\n",
    "    gc.collect()        # ask Python to free unreferenced memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6111038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume model_histories is a list of dicts, each with keys \"model_name\" and \"history\"\n",
    "def get_metric(history, metric_name):\n",
    "    if metric_name in history:\n",
    "        return history[metric_name]\n",
    "    for suffix in [\"_12\", \"_2\"]:\n",
    "        if f\"{metric_name}{suffix}\" in history:\n",
    "            return history[f\"{metric_name}{suffix}\"]\n",
    "    raise KeyError(f\"Metric {metric_name} not found in history.\")\n",
    "\n",
    "for model_entry in model_histories:\n",
    "    name = model_entry[\"model_name\"]\n",
    "    history = model_entry[\"history\"].history  # Keras history object\n",
    "\n",
    "    epoch_losses = get_metric(history, 'loss')\n",
    "    epoch_val_losses = get_metric(history, 'val_loss')\n",
    "    epoch_accuracies = get_metric(history, 'auc')\n",
    "    epoch_val_accuracies = get_metric(history, 'val_auc')\n",
    "    epoch_precisions = get_metric(history, 'precision')\n",
    "    epoch_val_precisions = get_metric(history, 'val_precision')\n",
    "    epoch_recalls = get_metric(history, 'recall')\n",
    "    epoch_val_recalls = get_metric(history, 'val_recall')\n",
    "\n",
    "    # Calculate F1 scores\n",
    "    def safe_f1(p, r):\n",
    "        return 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "    epoch_f1s = [safe_f1(p, r) for p, r in zip(epoch_precisions, epoch_recalls)]\n",
    "    epoch_val_f1s = [safe_f1(p, r) for p, r in zip(epoch_val_precisions, epoch_val_recalls)]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epoch_losses, 'b--o', label='Training Loss')\n",
    "    plt.plot(epoch_val_losses, 'r--o', label='Validation Loss')\n",
    "    plt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epoch_accuracies, '--o', label='Training AUC', color='#ff7f0e')\n",
    "    plt.plot(epoch_val_accuracies, 'r--o', label='Validation AUC')\n",
    "    plt.title('AUC'); plt.xlabel('Epoch'); plt.ylabel('AUC'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epoch_precisions, 'g--o', label='Training Precision')\n",
    "    plt.plot(epoch_val_precisions, 'r--o', label='Validation Precision')\n",
    "    plt.title('Precision'); plt.xlabel('Epoch'); plt.ylabel('Precision'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epoch_recalls, 'c--o', label='Training Recall')\n",
    "    plt.plot(epoch_val_recalls, 'r--o', label='Validation Recall')\n",
    "    plt.title('Recall'); plt.xlabel('Epoch'); plt.ylabel('Recall'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(epoch_f1s, 'm--o', label='Training F1')\n",
    "    plt.plot(epoch_val_f1s, 'r--o', label='Validation F1')\n",
    "    plt.title('F1 Score'); plt.xlabel('Epoch'); plt.ylabel('F1'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.suptitle(f\"Training Progress - {name}\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(f'./figures/training_{ARCH_NAME}_{name}_{VERSION}.png', dpi=500, bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
