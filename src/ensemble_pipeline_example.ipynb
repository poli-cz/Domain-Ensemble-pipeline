{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classification Pipeline Example\n",
    "\n",
    "This notebook demonstrates how to use the trained domain classification pipeline to make predictions.  \n",
    "It supports both label prediction and probability estimation, with optional SHAP explanations.  \n",
    "The first section shows how to classify a small batch of domains interactively;  \n",
    "the second one computes performance metrics across the entire test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "\n",
    "from core.validator import load_saved_split, load_train_split, load_random_sample\n",
    "from pipeline import DomainClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set label and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALICIOUS_LABEL = \"phishing\"  # phishing / malware\n",
    "STAGE = 3                     # 1 / 2 / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification pipeline demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load saved verification data\n",
    "x_test, y_test = load_random_sample(STAGE, MALICIOUS_LABEL, folder=\"./data/\")\n",
    "\n",
    "# Initialize classifier\n",
    "DomainClassifier = DomainClassifier(data_sample=x_test, label=MALICIOUS_LABEL)\n",
    "DomainClassifier.determine_stage(x_test)\n",
    "\n",
    "# Initialize confusion matrix counters\n",
    "fp, fn, tp, tn = 0, 0, 0, 0\n",
    "\n",
    "# Classify domains one by one\n",
    "for domain, expected_label in zip(x_test, y_test):\n",
    "    # Get final prediction probability\n",
    "    final_class = DomainClassifier.classify_proba(domain)['final_proba']\n",
    "    print(f\"Final Class: {final_class}\")\n",
    "    input()  # Pause for user review (optional)\n",
    "\n",
    "    # Compare prediction with true label and update confusion matrix\n",
    "    if expected_label == 1 and final_class > 0.5:\n",
    "        tp += 1\n",
    "    elif expected_label == 1 and final_class <= 0.5:\n",
    "        fn += 1\n",
    "    elif expected_label == 0 and final_class > 0.5:\n",
    "        fp += 1\n",
    "    elif expected_label == 0 and final_class <= 0.5:\n",
    "        tn += 1\n",
    "    else:\n",
    "        # Handle unexpected case\n",
    "        print(\"Error in classification\")\n",
    "        print(domain, expected_label, final_class)\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure pipeline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from core.validator import load_saved_split\n",
    "from pipeline import DomainClassifier\n",
    "\n",
    "# Configuration\n",
    "STAGE = 1\n",
    "VERIFICATION = True\n",
    "LABELS = [\"phishing\", \"malware\"]\n",
    "SAVE_PATH = \"tex_sources/pipeline_verif.tex\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"tex_sources\", exist_ok=True)\n",
    "\n",
    "# Initialize list for storing results\n",
    "results = []\n",
    "\n",
    "# Evaluate each label separately\n",
    "for label in LABELS:\n",
    "    print(f\"\\n=== Evaluating {label.upper()} (Stage {STAGE}) ===\")\n",
    "\n",
    "    # Load verification data for given label\n",
    "    x_data, y_data = load_saved_split(STAGE, label, folder=\"./data/\", verification=VERIFICATION)\n",
    "    x_data = x_data[:10000]  # Limit data for performance\n",
    "    y_data = y_data[:10000]\n",
    "\n",
    "    # Initialize classifier\n",
    "    clf = DomainClassifier(data_sample=x_data, label=label)\n",
    "    clf.determine_stage(x_data)\n",
    "\n",
    "    # Initialize metrics\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_proba = []\n",
    "\n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Perform classification\n",
    "    for x, true_label in zip(x_data, y_data):\n",
    "        result = clf.classify_proba(x)\n",
    "        final_proba = result[\"final_proba\"]\n",
    "        pred_label = 1 if final_proba > 0.5 else 0\n",
    "\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(pred_label)\n",
    "        y_proba.append(final_proba)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    speed = len(y_true) / elapsed\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "    # Store results\n",
    "    stage_results = {\n",
    "        \"Stage\": STAGE,\n",
    "        \"Label\": label,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC AUC\": auc,\n",
    "        \"Domains/sec\": round(speed, 2),\n",
    "    }\n",
    "\n",
    "    results.append(stage_results)\n",
    "    print(f\"Stage results: {stage_results}\")\n",
    "\n",
    "# Export results to LaTeX table\n",
    "with open(SAVE_PATH, \"w\") as f:\n",
    "    for result in results:\n",
    "        f.write(f\"\"\"\\\\begin{{table}}[H]\n",
    "                    \\\\centering\n",
    "                    \\\\begin{{tabular}}{{|l|c|}}\n",
    "                    \\\\hline\n",
    "                    \\\\textbf{{Metrika}} & \\\\textbf{{{result['Label'].capitalize()}}} \\\\\\\\\n",
    "                    \\\\hline\n",
    "                    Přesnost (Accuracy) & \\\\texttt{{{result['Accuracy']:.4f}}} \\\\\\\\\n",
    "                    Precision (Přesnost) & \\\\texttt{{{result['Precision']:.4f}}} \\\\\\\\\n",
    "                    Recall (Úplnost) & \\\\texttt{{{result['Recall']:.4f}}} \\\\\\\\\n",
    "                    F1 Skóre & \\\\texttt{{{result['F1 Score']:.4f}}} \\\\\\\\\n",
    "                    ROC AUC & \\\\texttt{{{result['ROC AUC']:.4f}}} \\\\\\\\\n",
    "                    \\\\hline\n",
    "                    \\\\end{{tabular}}\n",
    "                    \\\\caption{{Výsledky klasifikace {result['Label']} domén – verifikační sada}}\n",
    "                    \\\\label{{tab:final_pipeline_ver_{result['Label']}_{result['Stage']}}}\n",
    "                    \\\\end{{table}}\n",
    "                    \"\"\")\n",
    "\n",
    "print(f\"Saved LaTeX results to: {SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
