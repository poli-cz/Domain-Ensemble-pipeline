{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Validation and Metrics\n",
    "\n",
    "This notebook evaluates trained models using a labeled dataset and reports precision, recall, confusion matrix, and F1-score. It optionally supports multiple stages and SHAP value analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.validator import ModelValidator, load_saved_split, load_train_split\n",
    "from models.model_wrapper import ModelWrapper\n",
    "\n",
    "model_wrapper = ModelWrapper(model_dir=\"models\")\n",
    "\n",
    "### CONFIGURATION\n",
    "ARCH_NAME = \"Lgbm\"           # cnn / XgBoost / Lgbm / feedforward / svm\n",
    "VERSION = \"v1.1\"             # v1.0 / v1.1\n",
    "malicious_label = \"phishing\" # malware / phishing \n",
    "stage = 3                    # 1 / 2 / 3 \n",
    "verification = True          # True / False\n",
    "\n",
    "\n",
    "prefix=f\"stage_{stage}\"\n",
    "model = model_wrapper.load(arch_name=ARCH_NAME, label=malicious_label, prefix=prefix, version=VERSION)\n",
    "x_test, y_test = load_saved_split(stage, label=malicious_label, folder=\"./data/\", verification=verification)\n",
    "\n",
    "# Initialize validator class\n",
    "validator = ModelValidator(model, x_test, y_test, arch_name=ARCH_NAME, label=malicious_label, prefix=f\"stage_{stage}\", version=VERSION, verification=verification, stage=stage)  \n",
    "\n",
    "# Evaluate performance\n",
    "validator.evaluate_performance(save_results = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Verification / Validation for all models \n",
    "This script can be used to bulk evaluate all models withing this pipeline. Adjust list of architectures, list of stages and target dataset (verification/validation). Overall, the script will:\n",
    "- Generates Confusion Matrices\n",
    "- Generates Classification Reports\n",
    "- Generates .TEX table and all the measurement data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = ModelWrapper(model_dir=\"models\")\n",
    "\n",
    "# Define architectures you wish to bulk test\n",
    "architectures = [\"XgBoost\" \"feedforward\", \"Lgbm\", \"cnn\"]\n",
    "\n",
    "malicious_labels = [\"phishing\"]\n",
    "stages = [1, 2, 3]\n",
    "VERSION = \"v1.1\"\n",
    "verification = False\n",
    "\n",
    "\n",
    "# Iteration over all combinations of architectures, malicious labels, and stages...\n",
    "for architecture in architectures:       \n",
    "    for malicious_label in malicious_labels:\n",
    "        for stage in stages:\n",
    "            model = model_wrapper.load(arch_name=architecture, label=malicious_label, prefix=f\"stage_{stage}\", version=VERSION)\n",
    "            x_test, y_test = load_saved_split(stage, malicious_label, folder=\"./data/\", verification=verification)\n",
    "            \n",
    "            # Initialize validator class and pass model for evaluation\n",
    "            validator = ModelValidator(model, x_test, y_test, arch_name=architecture, label=malicious_label, prefix=f\"stage_{stage}\", version=VERSION, verification=verification, stage=stage)  \n",
    "            \n",
    "            # Validator saves .tex tables, cf matrices and performance metrics to \n",
    "            validator.evaluate_performance()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
