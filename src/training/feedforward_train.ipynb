{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da08d06-be0d-468a-bd17-d7bcba74880a",
   "metadata": {},
   "source": [
    "## Columns to be removed from training/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715b0f7",
   "metadata": {},
   "source": [
    "# Load Tensorflow and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a3b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:29:12.380006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-02 22:29:12.380036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-02 22:29:12.381340: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-02 22:29:12.387634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-02 22:29:13.091512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:29:13.864802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:29:13.904248: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:29:13.907051: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:29:14.923968: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:29:14.925608: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:29:14.927000: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:29:14.928258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 2130 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from core.loader import Loader\n",
    "from models.model_wrapper import ModelWrapper\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(device.physical_device_desc)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1258a0e",
   "metadata": {},
   "source": [
    "# Load input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069058d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_dataset_filenames = [\n",
    "    'parkets/benign/benign_2312_anonymized_HTML.parquet', \n",
    "    'parkets/benign/umbrella_benign_FINISHED_HTML.parquet'\n",
    "        \n",
    "]\n",
    "malicious_dataset_filenames = [\n",
    "    'parkets/malware_2406_strict_HTML.parquet'\n",
    "]\n",
    "\n",
    "\n",
    "# print me number of domains from each separate dataset\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "benign_label = \"benign\"\n",
    "malicious_label = \"phishing\"\n",
    "\n",
    "class_map = {benign_label: 0, malicious_label: 1}\n",
    "# print labels from malicious datasets\n",
    "\n",
    "loader = Loader(benign_dataset_filenames, malicious_dataset_filenames, benign_label=benign_label, malicious_label=malicious_label, subsample=1.0)\n",
    "df = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca51639",
   "metadata": {},
   "source": [
    "# split into 3 stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b37550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n",
      "/home/poli/Desktop/git/deep_domain_detection/src/core/loader.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"label\"] = self.df[\"label\"].copy()\n"
     ]
    }
   ],
   "source": [
    "from core.loader import Segmenter\n",
    "\n",
    "# Define the aggregates that needs to be created\n",
    "\n",
    "aggregates = [\n",
    "    [\"lex_\"],\n",
    "    [\"lex_\", \"dns_\", \"ip_\", \"geo_\"],\n",
    "    [\"lex_\", \"dns_\", \"ip_\", \"tls_\", \"geo_\", \"rdap_\"],\n",
    "]\n",
    "\n",
    "segmenter = Segmenter(df)\n",
    "segmenter.create_base_subsets() # create base subsets\n",
    "segmenter.create_aggregated_subsets(aggregates)\n",
    "subset_dfs = segmenter.get_aggregated_subsets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e103a79",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87705691",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['label'].apply(lambda x: class_map[x]) # y vector\n",
    "features = df.drop('label', axis=1).copy() # X matrix\n",
    "\n",
    "features = loader.scale(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fd630",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "  features,\n",
    "  labels,\n",
    "  test_size=0.2,\n",
    "  random_state=42,\n",
    "  shuffle=True, \n",
    "  stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecd4d4",
   "metadata": {},
   "source": [
    "# Define the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ecaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# TODO: Testovat NN na prvni 3 stupne. \n",
    "# TODO: Pro každý stupeň vybrat nejlepší model\n",
    "\n",
    "# TODO: Měřit na celém datasetu + potom udělat validaci na CLF testu\n",
    "\n",
    "\n",
    "ARCH_NAME = \"feedforward\"\n",
    "VERSION = \"v1.0\"\n",
    "LR = 0.0023\n",
    "\n",
    "def build_feedforward_net(feature_size):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(feature_size,))\n",
    "    \n",
    "    # First hidden layer\n",
    "    \n",
    "    x = Dense(1024, activation=None)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    x = Dense(512, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Third hidden layer\n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Fourth hidden layer\n",
    "    x = Dense(128, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build and return the model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"ARCH_NAME\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save subset_dfs to pkl file to tmp folder\n",
    "import os\n",
    "import tempfile\n",
    "import pickle\n",
    "import torch\n",
    "from core.loader import Loader\n",
    "from models.model_wrapper import ModelWrapper\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(device.physical_device_desc)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_file = \"subset_dfs_malware.pkl\"\n",
    "tmp_dir_name = \"tmp\"\n",
    "\n",
    "\n",
    "tmp_dir = os.path.join(os.getcwd(), tmp_dir_name)\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)\n",
    "tmp_file = os.path.join(tmp_dir, save_file)\n",
    "\n",
    "def save_subset_dfsp(subset_dfs):\n",
    "    with open(tmp_file, 'wb') as f:\n",
    "        pickle.dump(subset_dfs, f)\n",
    "    return tmp_file\n",
    "\n",
    "def load_subset_dfs_from_tmp(tmp_file):\n",
    "    with open(tmp_file, 'rb') as f:\n",
    "        subset_dfs = pickle.load(f)\n",
    "    return subset_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed10662",
   "metadata": {},
   "source": [
    "### Save subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dcd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save subsets\n",
    "save_subset_dfsp(subset_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06670977",
   "metadata": {},
   "source": [
    "### Load subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subsets \n",
    "subset_dfs = load_subset_dfs_from_tmp(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a587cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:32:03.726051: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:32:03.733782: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:32:03.736679: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training Feedforward NN on 'lex_agg' features…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:32:33.429815: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:32:33.433003: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:32:33.435792: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:32:33.447119: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:32:33.449848: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-02 22:32:33.452528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2130 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-05-02 22:32:39.289586: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 363918176 exceeds 10% of free system memory.\n",
      "2025-05-02 22:32:39.647333: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 363918176 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:33:10.155717: I external/local_xla/xla/service/service.cc:168] XLA service 0x7780d4017ae0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-05-02 22:33:10.155741: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2025-05-02 22:33:10.175511: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-05-02 22:33:10.333913: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746217990.419957 1131486 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434/1434 [==============================] - 39s 6ms/step - loss: 0.2223 - precision: 0.7019 - recall: 0.3401 - auc: 0.8931 - val_loss: 0.2176 - val_precision: 0.6850 - val_recall: 0.4001 - val_auc: 0.8983\n",
      "Epoch 2/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.2051 - precision: 0.7331 - recall: 0.3937 - auc: 0.9110 - val_loss: 0.2027 - val_precision: 0.7241 - val_recall: 0.4285 - val_auc: 0.9135\n",
      "Epoch 3/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1993 - precision: 0.7412 - recall: 0.4094 - auc: 0.9166 - val_loss: 0.1987 - val_precision: 0.7618 - val_recall: 0.3880 - val_auc: 0.9172\n",
      "Epoch 4/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1961 - precision: 0.7458 - recall: 0.4183 - auc: 0.9198 - val_loss: 0.1975 - val_precision: 0.7155 - val_recall: 0.4563 - val_auc: 0.9185\n",
      "Epoch 5/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1928 - precision: 0.7520 - recall: 0.4284 - auc: 0.9228 - val_loss: 0.1967 - val_precision: 0.7435 - val_recall: 0.4245 - val_auc: 0.9193\n",
      "Epoch 6/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1899 - precision: 0.7529 - recall: 0.4372 - auc: 0.9257 - val_loss: 0.1957 - val_precision: 0.7255 - val_recall: 0.4476 - val_auc: 0.9210\n",
      "Epoch 7/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1876 - precision: 0.7546 - recall: 0.4438 - auc: 0.9276 - val_loss: 0.1935 - val_precision: 0.7360 - val_recall: 0.4501 - val_auc: 0.9221\n",
      "Epoch 8/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1852 - precision: 0.7580 - recall: 0.4483 - auc: 0.9298 - val_loss: 0.1931 - val_precision: 0.6886 - val_recall: 0.5176 - val_auc: 0.9237\n",
      "Epoch 9/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1831 - precision: 0.7610 - recall: 0.4560 - auc: 0.9316 - val_loss: 0.1910 - val_precision: 0.7517 - val_recall: 0.4449 - val_auc: 0.9242\n",
      "Epoch 10/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.1812 - precision: 0.7627 - recall: 0.4622 - auc: 0.9333 - val_loss: 0.1904 - val_precision: 0.7680 - val_recall: 0.4322 - val_auc: 0.9248\n",
      "Epoch 11/25\n",
      "1434/1434 [==============================] - 8s 6ms/step - loss: 0.1787 - precision: 0.7680 - recall: 0.4669 - auc: 0.9354 - val_loss: 0.1918 - val_precision: 0.7596 - val_recall: 0.4414 - val_auc: 0.9241\n",
      "Epoch 12/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.1771 - precision: 0.7688 - recall: 0.4717 - auc: 0.9367 - val_loss: 0.1928 - val_precision: 0.7119 - val_recall: 0.4965 - val_auc: 0.9238\n",
      "Epoch 13/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1753 - precision: 0.7680 - recall: 0.4779 - auc: 0.9383 - val_loss: 0.1903 - val_precision: 0.7550 - val_recall: 0.4534 - val_auc: 0.9253\n",
      "Epoch 14/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1728 - precision: 0.7757 - recall: 0.4806 - auc: 0.9403 - val_loss: 0.1905 - val_precision: 0.7432 - val_recall: 0.4708 - val_auc: 0.9247\n",
      "Epoch 15/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1714 - precision: 0.7749 - recall: 0.4874 - auc: 0.9415 - val_loss: 0.1923 - val_precision: 0.7347 - val_recall: 0.4769 - val_auc: 0.9242\n",
      "Epoch 16/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1688 - precision: 0.7773 - recall: 0.4951 - auc: 0.9434 - val_loss: 0.1925 - val_precision: 0.7506 - val_recall: 0.4557 - val_auc: 0.9244\n",
      "Epoch 17/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1667 - precision: 0.7792 - recall: 0.4986 - auc: 0.9450 - val_loss: 0.1929 - val_precision: 0.7491 - val_recall: 0.4535 - val_auc: 0.9241\n",
      "Epoch 18/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1656 - precision: 0.7800 - recall: 0.5049 - auc: 0.9458 - val_loss: 0.1938 - val_precision: 0.7487 - val_recall: 0.4678 - val_auc: 0.9237\n",
      "Epoch 19/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1630 - precision: 0.7820 - recall: 0.5117 - auc: 0.9478 - val_loss: 0.1965 - val_precision: 0.7281 - val_recall: 0.4717 - val_auc: 0.9228\n",
      "Epoch 20/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1613 - precision: 0.7843 - recall: 0.5164 - auc: 0.9490 - val_loss: 0.1953 - val_precision: 0.7427 - val_recall: 0.4682 - val_auc: 0.9234\n",
      "Epoch 21/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1600 - precision: 0.7854 - recall: 0.5196 - auc: 0.9500 - val_loss: 0.1997 - val_precision: 0.7512 - val_recall: 0.4521 - val_auc: 0.9212\n",
      "Epoch 22/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1578 - precision: 0.7879 - recall: 0.5249 - auc: 0.9515 - val_loss: 0.2000 - val_precision: 0.7384 - val_recall: 0.4713 - val_auc: 0.9212\n",
      "Epoch 23/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.1559 - precision: 0.7911 - recall: 0.5292 - auc: 0.9529 - val_loss: 0.2035 - val_precision: 0.7374 - val_recall: 0.4770 - val_auc: 0.9212\n",
      "Saving as stage: stage_1\n",
      "\n",
      "🚀 Training Feedforward NN on 'lex_+dns_+ip_+geo_agg' features…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:37:10.800631: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 751314944 exceeds 10% of free system memory.\n",
      "2025-05-02 22:37:11.583246: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 751314944 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1434/1434 [==============================] - ETA: 0s - loss: 0.1080 - precision: 0.8245 - recall: 0.7629 - auc: 0.9786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:37:23.292705: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 187829248 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434/1434 [==============================] - 12s 7ms/step - loss: 0.1080 - precision: 0.8245 - recall: 0.7629 - auc: 0.9786 - val_loss: 0.0928 - val_precision: 0.8871 - val_recall: 0.7531 - val_auc: 0.9839\n",
      "Epoch 2/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.0857 - precision: 0.8611 - recall: 0.8092 - auc: 0.9863 - val_loss: 0.0880 - val_precision: 0.8582 - val_recall: 0.8042 - val_auc: 0.9851\n",
      "Epoch 3/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.0803 - precision: 0.8685 - recall: 0.8239 - auc: 0.9879 - val_loss: 0.0796 - val_precision: 0.8565 - val_recall: 0.8448 - val_auc: 0.9871\n",
      "Epoch 4/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.0738 - precision: 0.8792 - recall: 0.8370 - auc: 0.9896 - val_loss: 0.0761 - val_precision: 0.8641 - val_recall: 0.8531 - val_auc: 0.9892\n",
      "Epoch 5/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0703 - precision: 0.8839 - recall: 0.8439 - auc: 0.9906 - val_loss: 0.0724 - val_precision: 0.8875 - val_recall: 0.8411 - val_auc: 0.9890\n",
      "Epoch 6/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.0699 - precision: 0.8865 - recall: 0.8452 - auc: 0.9906 - val_loss: 0.0713 - val_precision: 0.9025 - val_recall: 0.8274 - val_auc: 0.9894\n",
      "Epoch 7/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.0639 - precision: 0.8973 - recall: 0.8565 - auc: 0.9921 - val_loss: 0.0716 - val_precision: 0.8783 - val_recall: 0.8517 - val_auc: 0.9897\n",
      "Epoch 8/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0619 - precision: 0.8992 - recall: 0.8616 - auc: 0.9926 - val_loss: 0.0686 - val_precision: 0.8877 - val_recall: 0.8572 - val_auc: 0.9902\n",
      "Epoch 9/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0606 - precision: 0.9034 - recall: 0.8633 - auc: 0.9930 - val_loss: 0.0673 - val_precision: 0.8968 - val_recall: 0.8547 - val_auc: 0.9905\n",
      "Epoch 10/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0586 - precision: 0.9063 - recall: 0.8687 - auc: 0.9935 - val_loss: 0.0673 - val_precision: 0.8909 - val_recall: 0.8630 - val_auc: 0.9904\n",
      "Epoch 11/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0552 - precision: 0.9115 - recall: 0.8753 - auc: 0.9941 - val_loss: 0.0674 - val_precision: 0.9015 - val_recall: 0.8549 - val_auc: 0.9893\n",
      "Epoch 12/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0537 - precision: 0.9136 - recall: 0.8788 - auc: 0.9945 - val_loss: 0.0700 - val_precision: 0.8854 - val_recall: 0.8799 - val_auc: 0.9912\n",
      "Epoch 13/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0518 - precision: 0.9165 - recall: 0.8829 - auc: 0.9949 - val_loss: 0.0669 - val_precision: 0.9083 - val_recall: 0.8509 - val_auc: 0.9896\n",
      "Epoch 14/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0498 - precision: 0.9199 - recall: 0.8882 - auc: 0.9953 - val_loss: 0.0670 - val_precision: 0.9099 - val_recall: 0.8519 - val_auc: 0.9891\n",
      "Epoch 15/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.0476 - precision: 0.9223 - recall: 0.8942 - auc: 0.9957 - val_loss: 0.0674 - val_precision: 0.9055 - val_recall: 0.8647 - val_auc: 0.9890\n",
      "Epoch 16/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0464 - precision: 0.9243 - recall: 0.8964 - auc: 0.9959 - val_loss: 0.0676 - val_precision: 0.9043 - val_recall: 0.8614 - val_auc: 0.9889\n",
      "Epoch 17/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0445 - precision: 0.9262 - recall: 0.9005 - auc: 0.9962 - val_loss: 0.0673 - val_precision: 0.9116 - val_recall: 0.8583 - val_auc: 0.9890\n",
      "Epoch 18/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0430 - precision: 0.9287 - recall: 0.9037 - auc: 0.9966 - val_loss: 0.0700 - val_precision: 0.8979 - val_recall: 0.8786 - val_auc: 0.9874\n",
      "Epoch 19/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0414 - precision: 0.9306 - recall: 0.9082 - auc: 0.9968 - val_loss: 0.0703 - val_precision: 0.9072 - val_recall: 0.8698 - val_auc: 0.9867\n",
      "Epoch 20/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0397 - precision: 0.9340 - recall: 0.9122 - auc: 0.9970 - val_loss: 0.0734 - val_precision: 0.8984 - val_recall: 0.8721 - val_auc: 0.9855\n",
      "Epoch 21/25\n",
      "1434/1434 [==============================] - 8s 6ms/step - loss: 0.0384 - precision: 0.9356 - recall: 0.9165 - auc: 0.9972 - val_loss: 0.0781 - val_precision: 0.8922 - val_recall: 0.8807 - val_auc: 0.9848\n",
      "Epoch 22/25\n",
      "1434/1434 [==============================] - 8s 5ms/step - loss: 0.0370 - precision: 0.9377 - recall: 0.9193 - auc: 0.9974 - val_loss: 0.0747 - val_precision: 0.9052 - val_recall: 0.8738 - val_auc: 0.9859\n",
      "Epoch 23/25\n",
      "1434/1434 [==============================] - 7s 5ms/step - loss: 0.0362 - precision: 0.9390 - recall: 0.9210 - auc: 0.9975 - val_loss: 0.0759 - val_precision: 0.9081 - val_recall: 0.8600 - val_auc: 0.9860\n",
      "Saving as stage: stage_2\n",
      "We already have stage 3\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.model_wrapper import ModelWrapper\n",
    "\n",
    "\n",
    "model_histories = []\n",
    "\n",
    "# make sure TF only allocates as much GPU memory as it needs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "wrapper = ModelWrapper()\n",
    "\n",
    "i = 1\n",
    "for prefix, subset_df in subset_dfs.items():\n",
    "    if i == 3:\n",
    "        print(\"We already have stage 3\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\n🚀 Training Feedforward NN on '{prefix}' features…\")\n",
    "    labels   = subset_df['label'].map(class_map)\n",
    "    features = loader.scale(subset_df.drop('label', axis=1), stage=i, model=ARCH_NAME)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        features, labels,\n",
    "        test_size=0.2, random_state=42,\n",
    "        shuffle=True, stratify=labels\n",
    "    )\n",
    "\n",
    "    model = build_feedforward_net(X_train.shape[1])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LR),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['Precision', 'Recall', 'AUC']\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        batch_size=512,\n",
    "        epochs=15,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        class_weight={0: 1.0, 1: 0.8},\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "\n",
    "    model_histories.append({\"model_name\": prefix,\n",
    "                      \"history\": history})\n",
    "    \n",
    "\n",
    "    wrapper.save(model,\n",
    "                 arch_name=ARCH_NAME,\n",
    "                 label=malicious_label,\n",
    "                 prefix=prefix,\n",
    "                 version=VERSION)\n",
    "\n",
    "    # ---- here’s the magic ----\n",
    "    K.clear_session()    # drops the entire TF graph + variables\n",
    "    del model           # remove the Python reference\n",
    "    del history         # free training history\n",
    "    del X_train, X_test, Y_train, Y_test, features, labels\n",
    "    gc.collect()        # ask Python to free unreferenced memory\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6111038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(history, metric_name):\n",
    "    if metric_name in history.history:\n",
    "        return history.history[metric_name]\n",
    "    elif f\"{metric_name}_12\" in history.history:\n",
    "        return history.history[f\"{metric_name}_12\"]\n",
    "    elif f\"{metric_name}_2\" in history.history:\n",
    "        return history.history[f\"{metric_name}_2\"]\n",
    "    else:\n",
    "        raise KeyError(f\"Metric {metric_name} not found in history.\")\n",
    "\n",
    "epoch_losses = get_metric(history, 'loss')\n",
    "epoch_val_losses = get_metric(history, 'val_loss')\n",
    "epoch_accuracies = get_metric(history, 'auc')\n",
    "epoch_val_accuracies = get_metric(history, 'val_auc')\n",
    "epoch_precisions = get_metric(history, 'precision')\n",
    "epoch_val_precisions = get_metric(history, 'val_precision')\n",
    "epoch_recalls = get_metric(history, 'recall')\n",
    "epoch_val_recalls = get_metric(history, 'val_recall')\n",
    "\n",
    "# Calculate F1 score\n",
    "epoch_f1s = [2 * (p * r) / (p + r) for p, r in zip(epoch_precisions, epoch_recalls)]\n",
    "epoch_val_f1s = [2 * (p * r) / (p + r) for p, r in zip(epoch_val_precisions, epoch_val_recalls)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "# Plot for Loss\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(epoch_losses, linestyle='--', marker='o', color='b', label='Training Loss')\n",
    "plt.plot(epoch_val_losses, linestyle='--', marker='o', color='r', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for AUC (as a proxy for Accuracy)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(epoch_accuracies, linestyle='--', marker='o', color='#ff7f0e', label='Training AUC')\n",
    "plt.plot(epoch_val_accuracies, linestyle='--', marker='o', color='r', label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC (Accuracy)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for Precision\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(epoch_precisions, linestyle='--', marker='o', color='g', label='Training Precision')\n",
    "plt.plot(epoch_val_precisions, linestyle='--', marker='o', color='r', label='Validation Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for Recall\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(epoch_recalls, linestyle='--', marker='o', color='c', label='Training Recall')\n",
    "plt.plot(epoch_val_recalls, linestyle='--', marker='o', color='r', label='Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for F1 Score\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(epoch_f1s, linestyle='--', marker='o', color='m', label='Training F1 Score')\n",
    "plt.plot(epoch_val_f1s, linestyle='--', marker='o', color='r', label='Validation F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.suptitle('NN Training Progress', fontsize=16, y=1.02)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f'./figures/training_{ARCH_NAME}_{malicious_label}_{VERSION}.png', dpi=500, bbox_inches='tight', pad_inches=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb708ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the model\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ', loss_and_metrics[0])\n",
    "print('Accuracy = ', loss_and_metrics[1])\n",
    "\n",
    "# Generate predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = np.round(Y_pred).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# False Positive Rate\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "# Display the metrics\n",
    "print('\\n=== RESULTS ===')\n",
    "print(classification_report(Y_test, Y_pred, target_names=['Benign', 'Malicious'], digits=4))\n",
    "print('False Positive Rate =', fpr)\n",
    "\n",
    "\n",
    "# Display the confusion matrix\n",
    "print('\\nConfusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Optionally, plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))  # Increase figure size for better readability\n",
    "disp.plot(ax=ax, values_format='d')\n",
    "for labels in disp.text_:\n",
    "    for label in labels:\n",
    "        label.set_fontsize(18) \n",
    "plt.show()\n",
    "\n",
    "# TODO: ADD clf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3d6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze_feature_importance = True\n",
    "\n",
    "if analyze_feature_importance:\n",
    "    \n",
    "    import shap\n",
    "    \n",
    "    # Ensure that X_train and X_test are DataFrames with the correct column names\n",
    "    # You can set the column names from the 'features' DataFrame like this:\n",
    "    X_train.columns = features.columns\n",
    "    X_test.columns = features.columns\n",
    "    \n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Convert your training set to a NumPy format if it's not already\n",
    "    background = X_train[:n_samples].to_numpy()\n",
    "    \n",
    "    # Use the generic SHAP Explainer interface\n",
    "    explainer = shap.Explainer(model, background)\n",
    "    \n",
    "    # Generate SHAP values for the test set\n",
    "    shap_values = explainer(X_test[:n_samples].to_numpy(), max_evals=1000)\n",
    "    \n",
    "    # Plotting the summary plot for feature importance\n",
    "    # Use the column names from the 'features' DataFrame as the feature names\n",
    "    shap.summary_plot(shap_values.values, X_test[:n_samples], feature_names=features.columns, max_display=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
